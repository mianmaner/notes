## 机器学习

#### 绪论

1.基本概念：让计算机程序发现数据中的规律，并根据规律给出预测的一种智能技术

2.应用：CV、NLP、自动驾驶、手写识别、语音识别等

#### 基础知识

##### 1.基本术语

(1)
①分类：预测结果是离散值，例如“好瓜”“坏瓜”（逻辑回归、LDA、SVM、ANN、决策树、KNN$\cdots$）
②回归：预测结果是连续值，例如西瓜成熟度0.95，0.37
③聚类：预测结果是分组值

(2)
①监督学习（分类与回归）：有标记信息
②无监督学习（聚类）：无标记信息
③半监督学习：同时使用标记数据和未标记数据

(3)
①训练误差：在训练集上的误差
②泛化误差：在测试集上的误差

(4)
①过拟合：将训练集中不太一般的特性当作应具有的一般的性质
②欠拟合：一般性质尚未学好

(5)
经验风险：度量预测模型与训练数据的误差
结构风险：描述模型的某些性质，以避免过拟合

(6)
偏差：度量了学习算法的期望预测和真实结果的偏离程度，即刻画了算法的拟合能力
方差：度量了同样大小的训练集的变动所导致的学习性能的变化
噪声：表达了当前任务上任何学习算法所能达到的期望泛化误差的下界

> 低偏差代表经验风险最小化
> 低方差代表结构风险最小化

(7)泛化能力：机器学习对新鲜样本的适应能力

##### 2.评估方法

(1)
混淆矩阵：
True Positives(TP)    False Negtives(FN)
False Positives(FP)   True Negtives(TN)

> sklearn.metircs.confusion_matrix

(2)
①查准率（Precision）：预测成功的正例占预测正例的比率，即$\frac{TP}{TP+FP}$
②查全率（Recall）：预测成功的正例占真实正例的比率，即$\frac{TP}{TP+FN}$

(3)
P-R曲线：纵坐标查准率，横坐标查全率
F1度量：$\frac{2TP}{2TP+FP+FN}$

(4)
①ROC（曲线）：
纵坐标为查全率，横坐标为预测失败的反例占真实反例的比率
偏向左上好，偏向右下不好
②AUC：ROC曲线下的面积，表示随机抽取一个true样本和false样本，分类器接受true样本高于接受false样本的概率
(AUC取值在0-1之间，AUC值越大，正确率越高)

(5)
交叉验证：将整个训练数据划分成n份，选取其中一份作为验证数据集，其余n-1份作为训练数据集，进行轮换让每一份数据都有机会作为验证数据集

> sklearn.model_selection.cross_val_score

(5)
常用作损失函数：

==①交叉熵（Cross Entropy）：主要用于度量两组数据分布间的差异性
公式：$H(x,y)=-\sum P(x)logQ(x)$
(其中，P(x) 是随机变量 X 取值为 x 的真实分布概率，Q(x) 是随机变量 X 取值为 x 的模型预测概率)==

==②信息熵（Information Entropy）：主要用于度量样本集合纯度
公式：$H(x)=-\sum P(x)logP(x)$
信息增益：表示特征X使得类Y的不确定性减少的程度==

③信息增益率：对信息增益进行改进，考虑了自身熵

==④CART：使用Gini系数当作衡量标准
Gini系数：$Gini(p)=\sum_{k=1}^K p_k(1-p_k)=1-\sum_{k=1}^Kp_k^2$==

(6)
代价矩阵：产生错误预测的代价，可以在训练中动态调整代价矩阵
<img src="https://s2.loli.net/2023/07/31/7berBAgOfcFXDHW.png" alt="image-20230610104238574" style="zoom: 67%;" />

(7)
学习曲线：反映模型的学习程度，横坐标是训练数据规模，纵坐标是误差；有两条曲线，一个是训练集误差，一个是验证集误差
①当验证集和训练集的误差几乎相等且都很大，此时为欠拟合
②当验证集误差远大于训练集误差，此时为过拟合

(8)
PAC学习

> 弱学习算法：识别错误率小于1/2（即准确率比随机猜测略高）
> 强学习算法：识别准确率很高并能在多项式时间内完成的算法

弱学习算法可以提升为强学习算法，PAC辨识学习算法能从假设空间对中辨识概念类C，则称概念类C对假设空间而言是PAC可学习的

#### 线性回归

##### 1.线性回归

<img src="https://s2.loli.net/2023/07/31/wYtD3rGOP6B95j4.png" alt="image-20230527211850215" style="zoom: 33%;" />

(1)$h_{θ}(x)=θ_0+θ_1x_1+θ_2x_2=θ^Tx$（$\theta_0$是偏置项）

(2)对每个样本：$y^{(i)}=θ^Tx^{(i)}+\epsilon^{(i)}$（$\epsilon$为误差）

(3)误差：独立同分布，服从高斯分布
(4)代价函数：$J(\theta)=\frac{1}{2m}\displaystyle\sum_{i=1}^{n}(y^{(i)}-\theta^Tx^{(i)})^2$
(5)解析：

①由于误差服从高斯分布，$p(\epsilon^{(i)})=\frac{1}{\sqrt{2\pi}\sigma}exp(\frac{(\epsilon^{(i)})^2}{2\sigma^2})$

联立$y^{(i)}=θ^Tx+\epsilon^{(i)}$

得$p(θ)=\frac{1}{\sqrt{2\pi}\sigma}exp(\frac{(y^{(i)}-θ^Tx^{(i)})^2}{2\sigma^2})$

②要想θ参数使数据组合后为真实值，可以用极大似然估计法

似然函数$L(\theta)=\displaystyle\prod_{i=1}^n\frac{1}{\sqrt{2\pi}\sigma}exp(\frac{(y^{(i)}-θ^Tx^{(i)})^2}{2\sigma^2})$

将对数似然$logL(\theta)$展开化简，因为需要将似然函数最大化，得

==目标为最小化$J(\theta)=\frac{1}{2}\sum_{i=1}^{n}(y^{(i)}-\theta^Tx^{(i)})^2$====（这个式子就是最小二乘法）==

③对损失函数求偏导

$J(\theta)=\frac{1}{2}\sum_{i=1}^{n}(y^{(i)}-\theta^Tx^{(i)})^2=\frac{1}{2}(X\theta-y)^T(X\theta-y)$

(这里用到矩阵论里面的内容：对$\partial_\theta(\theta^TA\theta)$，若A为对称矩阵，则结果为$2A\theta$)

④关于代价函数

对于某一个样本代价函数$J(\theta)=\frac{1}{2}\displaystyle\sum_{i=1}^{n}(y^{(i)}-\theta^Tx^{(i)})^2$

==应用于一般情况多个样本，所以线性回归的代价函数$J(\theta)=\frac{1}{2m}\displaystyle\sum_{i=1}^{n}(y^{(i)}-\theta^Tx^{(i)})^2$==

##### ==2.梯度下降==

(1)![image-20230220111542737](https://s2.loli.net/2023/07/31/jKu4YoTIflUp6eN.png)

(2)教学PPT中的公式
![image-20230609095627987](https://s2.loli.net/2023/07/31/4JlmEKOYMkC8cti.png)(3)分类

==批量梯度下降（GD）==
==随机梯度下降（SGD）==

![image-20230527223027125](https://s2.loli.net/2023/07/31/LXZJftPhVFr5mBo.png)

注意：

①下降的方向一定为梯度的反方向，所以在求$\theta_j$时要加负号
②随机梯度下降相当于抽样一个进行（批量）梯度下降

==小批量梯度下降（BGD）（实际使用的方法）==

![image-20230527224131729](https://s2.loli.net/2023/07/31/C759syo6EZ3XKia.png)

(4)学习率α（步长）：对结果会产生巨大的影响，一般小一些
①$\alpha$过小会造成梯度下降太慢
②$\alpha$过大会造成梯度下降跳过代价函数最小值，这可能会造成收敛失败，甚至发散

#### 非线性回归

进行特征变换:

(1)不同情况（多项式回归）

对单元：$x^n+x^{n-1}+\cdots+x$
对多元：$x_1+x_2+x_1^2+x_2^2+x_1^2x_2\cdots$

(也可加上sinx、cosx等非线性表达特征)

(2)相关库函数

>  sklearn.preprocessing.PolynomialFeatures

(3)存在过拟合风险

解决方式：正则化

#### 逻辑回归

##### 1.二分类逻辑回归

(1)目的：分类还是回归？

==逻辑回归不是回归，是经典的二分类算法！==
(将连续的数值转化为对应的区间)

(2)Sigmoid函数（是线性分类器）

公式：$g(z)=\frac{1}{1+e^{-z}}$
效果：将任意的输入映射到了[0,1]区间

(3)解析：

①判决函数：$h_\theta(x)=g(\theta^Tx)$

②代价函数==$J(\theta)=-\frac{1}{m}logL(\theta)=-\frac{1}{m}\displaystyle\sum_{i=1}^m(y_ilogh_\theta(x_i)+(1-y_i)log(1-h_\theta(x_i)))$==

==代价函数的最小化（导数求导过程）==

(4)损失函数：交叉熵-信息熵（见基础知识）

##### 2.多分类逻辑回归

(1)实际上是多个二分类任务
(将连续的数值转化为对应的区间)

##### 3.线性判别分析（LDA）

![image-20230308150815745](https://s2.loli.net/2023/07/31/zXjHwO81atWvNcl.png)

(1)概念：使用超平面切分两类样本，使得两类样本类内距最小，类间距最大

(2)目标：类内距最小，类间距最大

(3)性质：LDA是有监督的；与PCA不同，更关心分类而不是方差

(4)算法：
①计算训练数据集中两类样本的均值（中心点）$\mu_0、\mu_1$和方差$\sum_0、\sum_1$
②计算类内散度$S_w$
③计算判决模型参数$w$
④计算判决模型阈值$y_0$
⑤最后得判决模型$y=w^Tx$与$y_0$作比较进行分类

#### 聚类方法

##### 1.聚类

目标：相似的东西分到一组
无监督学习方法：我们手里没有标签了

##### 2.K-MEANS算法

(1)想要得到簇的个数，需要指定K值

(2)质心：均值，即向量各维取平均即可

(3)优化目标：$min\sum_{i=1}^K\sum_{x\in C_1}dist(c_i,x)^2$
(让每一个样本到中心点的距离总和越小越好)

(4)工作流程：

①先随机初始化k个点，记作分类点

②计算每一个数据点与分类点的距离
距离小的归为一簇

③再更新各簇分类点为各簇的质心
重新进行步骤②，不断迭代

(5)可视化：
https://www.naftaliharris.com/blog/visualizing-k-means-clustering/

##### 3.DBSCAN聚类算法

(1)核心对象：
若某个点的密度达到算法设定的阈值即为核心点

(2)$\epsilon$-领域的距离阈值：
设定的半径r

(3)直接密度可达：
若某点p在点q的r领域内，且q是核心点，则p-q直接密度可达

(4)利用直接密度可达传递性
可得到密度可达（间接密度可达）

(5)噪声点：
不属于任何一个簇的点，从任何一个核心点出发都是不可达的

(6)可视化：
https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/

##### 4.评估指标

(1)Inertia:
每个样本点到中心点的距离

(2)轮廓系数
![image-20230529104100686](https://s2.loli.net/2023/07/31/7MzyvaPgdfJKVrp.png)

#### 决策树

##### 1.决策树

![image-20230529120407616](https://s2.loli.net/2023/07/31/HMilWZeCvyn1zSh.png)

(1)树模型：
从根节点一步步走到叶子结点
所有的数据最终都会落到叶子结点
既可以做分类也可以做回归

(2)树的组成

根节点：第一个选择点
非叶子节点与分支：中间过程
叶子结点：最终的决策结果

(3)如何切分特征（选择节点）
优先选择分类效果好的为根节点

- ==衡量标准：==

  ==信息熵（Entropy）
  公式：$H(x)=-\sum p_ilogp_i$（i=1,2,$\cdots$,n）
  信息增益：表示特征X使得类Y的不确定性减少的程度==

  信息增益率：对信息增益进行改进，考虑了自身熵

  ==CART：使用Gini系数当作衡量标准
  Gini系数：$Gini(p)=\sum_{k=1}^K p_k(1-p_k)=1-\sum_{k=1}^Kp_k^2$==

(4)剪枝策略

①原因：决策树过拟合风险很大
(可以理解为决策树性能太强，太能分了)

②预剪枝：
限制深度、叶子结点个数、叶子结点样本数、信息增益量等

③后剪枝：通过一定的衡量标准
$C_\alpha(T)=C(T)+\alpha|T_{leaf}|$
(T为衡量标准（比如gini））
(叶子结点越多，损失越大)

#### 集成学习

##### 1.集成学习

(1)Bagging：训练多个分类器取平均（代表算法：随机森林）

- ==随机森林==：是一种策略，适用于多分类、回归等任务

  随机：数据采样随机、特征选择随机
  森林：很多个决策树放在一起

(2)Boosting：从弱学习器开始加强，通过加权来进行训练

- ==AdaBoost算法==

  AdaBoost会根据前一次的分类效果调整数据权重
  解释：如果某一个数据在这一次分错了，下一次给予它更大的权重

(3)Stacking：聚合多个分类或回归模型 

- 堆叠：可以堆叠各种各样的分类器（KNN,SVM,RF等）

#### 支持向量机

> 简单点讲，SVM就是一种二分类模型，他的基本模型是的定义在特征空间上的**间隔最大**的线性分类器，SVM的学习策略就是间隔最大化。
>
> $\omega^Tx+b=0$ 决定一个超平面，我们先选择分离两类数据的两个平行超平面，然后最大化间隔 $\frac{2}{||w||}$
>
> 训练数据集的样本点中与分离超平面距离最近的数据点称为支持向量，支持向量是使 $y_i(X_i^TW+b)=1$ 的点，在决定最佳超平面时只有支持向量起作用，而其他数据点并不起作用，如果移动非支持向量，甚至删除非支持向量都不会对最优超平面产生任何影响
>
> 对偶问题：用 KKT 条件求解
>
> 软间隔就是给目标函数加上一个偏置项（惩罚项）
>
> 非线性 SVM 需要利用核函数将低微映射到高维可分

##### 1.支持向量机SVM

(1)解析：

①在样本空间中，可以通过如下线性方程来描述
划分超平面：$\omega^Tx+b=0$
(其中$\omega=(\omega_1;\omega_2;\cdots;\omega_d)$为超平面的法向量）

②样本空间中任一点到超平面的距离$r=\frac{|\omega^Tx+b|}{||\omega||}$
(向量除以向量的模即为它的方向)

最大间隔：两个异类支持向量到超平面的距离之和

③数据标签定义

- Y为样本的类别：X为正例的时候Y=+1，X为负例的时候Y=-1

- ①决策方程：$y(x)=\omega^T\phi(x)+b$
  (这里$\phi(x)$表示是对数据做了变换，暂时先将它当作x）

  ②判决函数（之一）
  $$ f(x)=\left\{ \begin{aligned} y(x_i) &\ge1 \quad\rightarrow y_i=+1 \\ y(x_i)&\le-1\quad\rightarrow y_i=-1  \end{aligned} \right. $$
  所以得到   $y_iy(x_i)>0$

④SVM的优化（最大化最小间隔）
<img src="https://s2.loli.net/2023/07/31/RTZ3tSiwznhLIBb.png" alt="image-20230611235817091" style="zoom:60%;" />



- 将点到平面距离变换为：$\frac{y_i(\omega^T\phi(x_i)+b)}{||\omega||}$
  (分子乘上$y_i$再去掉绝对值)

- ==优化目标：$argmax_{w,b}\{\frac{1}{||\omega||}min_i[y_i(\omega^T\phi(x_i)+b)]\}$==
  解释：
  $min_i[y_i(\omega^T\phi(x_i)+b)]$表示获取离划分平面最近的样本点
  $argmax_{\omega,b}$的意思是获取使得右式最大的参数$\omega,b$

- 经过放缩变换，可得到$y_i(\omega^T\phi(x_i)+b)$恒大于等于1
  所以只需要考虑$argmax_{w,b}\frac{1}{||\omega||}$（此即为目标函数）

  这等价于==$min_{\omega,b}||\omega||^2$==（这就是SVM的基本型）
  (约束条件：$y_i(\omega^T\phi(x_i)+b)\geq1$)

- 在约束条件下求极值$\rightarrow$拉格朗日乘子法
  $L(\omega,b,\alpha)=\frac{1}{2}||\omega||^2-\displaystyle\sum_{i=1}^n\alpha_i(y_i(\omega^T\phi(x_i)+b)-1)$

- 这里转化为“对偶问题”$\rightarrow$需满足KKT条件
  对$\omega$和$\alpha$求偏导等于0，再将结果代入原式
  得到$L(\omega,b,\alpha)=\displaystyle\sum_{i=1}^n\alpha_i-\frac{1}{2}\sum_{i=1,j=1}^n\alpha_i\alpha_jy_iy_j\phi^T(x_i)\phi(x_j)$
  (s.t：$\displaystyle\sum_{i=1}^m\alpha_iy_i=0$)
  这里的$\phi^T(x_i)\phi(x_j)$即为==核函数==

- 判决函数（最终）
  $f(x)=sign(w^Tx+b)=sign(\displaystyle\sum_{i=1}^m\alpha_iy_ix_ix+b)$

(2)软间隔：不需要完全分开，允许错误容忍

①引入松弛因子$\xi$==（考C大和C小对应的情况）==

- ![image-20230417103336582](https://s2.loli.net/2023/07/31/nUrSHhxAMLgp7Ft.png)
  新的目标函数：$min\frac{1}{2}||\omega||^2+C\displaystyle\sum_{i=1}^n\xi_i$
  当C很大时：意味着分类严格不能有错误
  当C很小时：意味着可以有更大的错误容忍

==②Hinge损失：$max(0,1-y_i(\omega^Tx+b))$==


(3)核变换

- 低维不可分问题
  目标：找到一种变换方法，也就是$\phi(x)$
  低维映射到高维
- 高斯核函数$K(X,Y)=exp\{-\frac{||X-Y||^2}{2\sigma^2}\}$

##### 2.统计学习理论

(1)模型的泛化能力：对于独立同分布的测试样本推断的能力

(2)VC维：统计学习理论的基础

#### 深度学习

> 深度学习是传统神经网络的升级——人工神经网络

##### 1.计算机视觉（CV）

==(1)KNN算法（k近邻算法）==

流程:
①计算已知类别数据集中的点与当前点的距离
②按照距离依次排序
③选取与当前点距离最小的K个点
④确定前K个点所在类别的出现概率
⑤返回前K个点出现频率最高的类别作为当前点预测分类

##### 2.自然语言处理（NLP）

(1)LSTM：长短期记忆递归神经网络
(2)Transformer：自然语言处理模型
(3)RNN：循环神经网络，一般用于情感分析

##### 3.华为MindSpore

(1)框架：
![image-20230609212526576](C:\Users\孙\AppData\Roaming\Typora\typora-user-images\image-20230609212526576.png)

#### 神经网络

##### 1.神经网络

(1)历史
<img src="https://s2.loli.net/2023/07/31/ow6jfXFyNIQzxme.png" alt="image-20230609205847757" />
①第一次寒冬（1980年）：单层感知器只能做很简单的任务，XOR异或问题不能解决，而多层神经网络发展没有希望。
②第二次寒冬（1993年）：只适用于特定场景的专家系统让政府失望，人工智能研究投入资金下调。

(2)M-P神经元模型
![神经网络与深度学习| xiangwei's blog](https://swordandtea.github.io/2020/03/13/machine_learning/neural_network&deep_learning/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/M-P%E7%A5%9E%E7%BB%8F%E5%85%83%E6%A8%A1%E5%9E%8B.jpeg)

①神经元将接收到的总输入与神经元的阈值进行比较，然后通过激活函数处理以产生神经元的输出
②图中的$f$指的是激活函数，激活函数有Sigmoid、Relu等

(3)神经网络架构
![image-20230531100232501](https://s2.loli.net/2023/07/31/kQZ4hxwe37KIyNB.png)
(输入层和输出层节点数是固定的，隐含层节点数不固定)

(4)感知机（Perceptron）

①感知机由两层神经元(输入层和输出层)组成，输出层是M-P神经元，是单层神经网络

②逻辑运算实现：根据$y=f(\sum_iw_ix_i-\theta)$，给$w$和$\theta$相应的值即可，可以实现逻辑与、逻辑或、逻辑非

③多层感知机可以实现逻辑异或：
![image-20230531095904220](https://s2.loli.net/2023/07/31/JCI9UDNZOqknlth.png)

(详见https://blog.csdn.net/weixin_42626736/article/details/117906192)

(5)前向传播

- 一张图理解![image-20230313102307778](https://s2.loli.net/2023/07/31/9mPI6CB8tzeFbDw.png)
  
  > 对于上图的解读：
  > ①$x$为初始输入量
  > ②$a_i^k$指的是第k层的第i个节点的输出，由第i个输入经过激活函数得到
  > ③$z_i^k$指的是第k层的第i个节点的输入，由权重与上一层的输出乘积得到（即红框里面的部分)
  > ④$\theta^k$是第k层的权重参数
  
- Softmax分类器
  归一化：$P(Y=k|X=x_i)=\frac{e^{s_k}}
  {\sum_je^{s_j}}$
  计算损失值：$L_i=-logP(Y=y_i|X=x_i)$

(6)反向传播

①链式、法则==（要会求导）==
![image-20230320103007754](https://s2.loli.net/2023/07/31/Ke2M5VfjZq4vWCc.png)
(这里$J$是最终输出，$\delta_j^{(l)}$为第l层的第j个节点的误差)

②对于一个样本和一个输出层节点的误差相对于模型参数的梯度：
<img src="https://s2.loli.net/2023/07/31/JbEzFyxAtp4scmr.png" alt="image-20230611234241234" style="zoom:50%;" />
③汇总整理（关于第一个式子的推导也要会）
![image-20230611234627575](https://s2.loli.net/2023/07/31/8QLKMfsgph9OmXo.png)

④过拟合解决方法：正则化（第一项无需正则化）

(7)代价函数
<img src="https://s2.loli.net/2023/07/31/ALIFPehObDWc5g9.png" alt="image-20230320101403822" style="zoom: 67%;" />

#### 贝叶斯分类器

##### 1.朴素贝叶斯分类器

(1)贝叶斯公式
<img src="https://s2.loli.net/2023/07/31/Xz3Oonq5RaIWLup.png" alt="image-20230515104209795" style="zoom: 50%;" />

>  $P(x|c)$一般会用$\displaystyle\prod_{i=1}^nP(x_i|c)$来计算

(2)判决函数
<img src="https://s2.loli.net/2023/07/31/9IijuAU4cYqESHk.png" alt="image-20230515105836751" style="zoom: 67%;" />

#### 网络机器学习

##### 1.PageRank

(1)Google把从A页面到B页面的链接解释为A页面给B页面进行投票

(2)带重启的随机游走：（知道含义即可）
<img src="https://s2.loli.net/2023/07/31/6ctBYRH7UXNVvGC.png" alt="image-20230610105950402" style="zoom: 67%;" />

（RWR算法迭代无穷多次后->得到收敛解）

#### 主成分分析（PCA）

1.主要用途：数据压缩，数据可视化，维度压缩

2.目标：删去重复特征，去除冗余特征，建立尽可能少的特征，使得这些新特征两两不相关

3.思路：通过线性组合现有特征实现组合后的方差最大化





