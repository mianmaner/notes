### Dataset Distillation（DD）

18年的论文，最早提出数据蒸馏的概念

**理论**

通常的梯度下降是小批量的 SGD，每次都需要从训练数据中选一个 minibatch 来更新。这篇文章的重点是学习到一个合成数据 $\hat x=\{\hat x_i\}_{i=1}^M$ 和学习率 $\hat \eta$，这样我们就可以固定梯度下降的函数，不需要选择 minibatch

方法是先给定一个初始参数，最小化以下目标函数来获得合成数据集和学习率
![image-20240517010555090](https://gitee.com/mianmann/drawing-bed-warehouse/raw/master/img/image-20240517010555090.png)

**初始参数设定**

随机初始化、固定初始化、随机预训练参数、固定预训练参数

### Dataset Condensation（DC）

首次提出通过梯度匹配策略来蒸馏数据集

本文希望学习一个带有参数 $\theta$ 的可微函数 $\phi$（如深度神经网络），以正确预测未知图像的标签，可以通过最小化训练集中的经验损失项来学习此函数的参数

**理论**

![image-20240517011238486](https://gitee.com/mianmann/drawing-bed-warehouse/raw/master/img/image-20240517011238486.png)

需要实现一样的泛化性能，就要实现参数 $\theta^S$ 和 $\theta^L$ 是相近的。这里提出了一个基于梯度匹配的方法，不仅希望最终参数接近，而且在整个优化过程遵循相似的路径

主要目标函数是让每一层的梯度函数距离函数（这里其实就是用两个向量的夹角余弦来距离）

$$d(A,B)=\sum_{i=1}^{out}(1-\frac{A_i\cdot B_i}{||A_i||||B_I||})$$

### Dataset Condensation with Differentiable Siamese Augmentation（DSA）

主要思路是将真实数据与合成数据使用相同的转换策略，通过数据增强将增强的知识转移到合成图像中（很棒的一个想法）

有效的同时利用数据增强和 DC

