### 正则化

(1)定义

正则化（Regularization）是机器学习中一种常用的技术，其主要目的是控制模型复杂度，减小过拟合。最基本的正则化方法是在原目标（代价）函数 中添加惩罚项，对复杂度高的模型进行“惩罚”。其数学表达形式为

![image-20240318152538328](https://gitee.com/mianmann/drawing-bed-warehouse/raw/master/img/image-20240318152538328.png)

最常用的 Ω 函数有两种，即 $ l1 $ 范数和 $l2$ 范数，相应称之为 $l1$ 正则化和 $l2$ 正则化。此时有：

![image-20240318152549841](https://gitee.com/mianmann/drawing-bed-warehouse/raw/master/img/image-20240318152549841.png)

(2)$l1$ 和 $l2$ 两种正则化的区别

$l1$ 正则化和 $l2$ 正则化的区别：①前者是权值向量绝对值之和，后者是平方和 ②前者的功能使权重稀疏，后者的功能使权重平滑

原因：圆形解空间更加平滑，菱形解空间更加稀疏
![img](https://ask.qcloudimg.com/http-save/yehe-2769421/7ukozc2ol3.jpeg)

(3)dropout（正则化的一种）

因为一层神经元中某些节点相比另外一些节点具有更强的表征能力，某层的神经元的节点在设定概率下失活（不进行前向传播）

(4)label smoothing（正则化的一种）

将硬标签转换成软标签，传统的one-hot编码标签会让模型缺乏适应性，对它的预测过于自信，在训练数据不足以覆盖所有情况的时候，就容易导致网络过拟合，泛化能力差

<img src="https://gitee.com/mianmann/drawing-bed-warehouse/raw/master/img/image-20240318165427848.png" alt="image-20240318165427848" style="zoom:50%;" />

其中K为多分类的类别总个数，α是一个较小的超参数（一般取0.1）从而加入了噪声，避免了模型过拟合，提升了模型的泛化能力

### 权重衰减

(1)定义

weight decay（权重衰减）是放在正则项前的一个系数，用于调节模型复杂度对损失函数的影响。在设置上，weight decay针对的是 $l2$ 正则化

(2)数值设置：一般weight decay比学习率低两到三个数量级

### BN和LN

(1)原理

我们在数据输入神经网络前会将数据进行标准化（因为理想输入的feature map需要均值为0，方差为1）但这仅仅满足了第一层网络的输入，第二层网络的输入就不一定满足这个feature map了。所以我们BN和LN的目的就是让我们的下一层网络的输入的feature map满足这一特点

(2)BN和LN的区别：BN是针对整个batch，而LN是针对batch中的每个输入（在NLP中一般使用LN）我举例说明一下：

```
我是中国人我爱中国
你们在干嘛呀000
大家好才是真的好0
```

上面的3条文本数据组成了一个batch，那么BN操作的时候会将3条文本中相同位置的字来做归一化处理（即‘我’‘你’‘大’），而这破坏了一句话的内在语义关系。但LN是将一句话做归一化处理，从这个角度看LN更适合NLP任务

