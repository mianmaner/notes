### 梯度下降法

#### BGD

批量梯度下降法，每进行一次参数更新，需要计算整个数据样本集，因此导致批量梯度下降法的速度会比较慢，尤其是数据集非常大的情况下，收敛速度就会非常慢，但是由于每次的下降方向为总体平均梯度，它得到的会是一个全局最优解

#### SGD

随机梯度下降法，不像BGD每一次参数更新，需要计算整个数据样本集的梯度，而是每次参数更新时，仅仅选取一个样本计算其梯度，训练速度快。由于每次迭代并不是都向着整体最优化方向，导致梯度下降的波动非常大，更容易从一个局部最优跳到另一个局部最优，准确度下降。

#### 小批量梯度下降法

小批量梯度下降法就是结合 BGD 和 SGD 的折中，对于含有 n 个训练样本的数据集，每次参数更新，选择一个大小为m（m<n） 的 mini-batch 数据样本计算其梯度。小批量梯度下降法即保证了训练的速度，又能保证最后收敛的准确率，目前的SGD默认是小批量梯度下降算法

### 动量优化法

#### Momentum

算法思想：参数更新时在一定程度上保留之前更新的方向，同时又利用当前batch的梯度微调最终的更新方向，简言之就是通过积累之前的动量来加速当前的梯度。在梯度方向改变时，momentum能够降低参数更新速度，从而减少震荡；在梯度方向相同时，momentum可以加速参数更新， 从而加速收敛。总而言之，momentum能够加速SGD收敛，抑制震荡

#### NAG

NAG是momentum的改进，在梯度更新时做一个矫正，具体做法就是在当前的梯度上添加上一时刻的动量

### 自适应学习率优化算法

#### AdaGrad

需要设置全局学习率 $\delta$，参数更新量为 $\Delta\theta=\delta\cdot\frac{1}{\sqrt{\sum_{i=1}^tg_i^2+\delta}}$（$g$ 是梯度）。在前期，梯度累计平方和比较小，参数更新量较大；随着迭代次数增多，梯度累计平方和也越来越大，参数更新量变小，这样就能缩小梯度

#### RMSprop

RMSProp算法修改了AdaGrad的梯度平方和累加为指数加权的移动平均，使得其在非凸设定下效果更好

#### Adam

Adam中动量直接并入了梯度一阶矩和二阶矩（指数加权）的估计。设置了默认参数值，并包括了偏置修正， 为不同的参数计算不同的自适应学习率