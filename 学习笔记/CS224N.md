## 词向量

### 传统方式

先探讨一个问题，我们怎么获取一个词语有用的含义？

以前常用的NLP解决方法，使用WordNet，这是一个包含同义词和上位词列表的同义词库

传统NLP中，我们用独热向量作为特征，这导致需要的特征向量维度过大，且由于独热向量都是正交的，所以词与词之间没有关联。

### Word2Vec

Word2Vec是一个学习词向量的框架，通过训练将每一个词映射成一个固定长度的向量，所有向量构成一个词向量空间，每一个向量（单词)可以看作是向量空间中的一个点，意思越相近的单词距离越近。

#### 基本概念

①分布式语义：一个词的含义由经常在他附近的词语提供。
②词向量：将出现在上下文中的单词视为向量，为每个单词建立稠密的向量，该向量在某种程度上表示该词的含义。

#### 主要思路

①我们有一个很大的语料库，在其中选择固定的词汇，为每个词自己创建向量。我们要为每个词找出好的向量表示。

②我们可以从一大堆文本中来进行训练预测，我们将中心词称为c，上下文词称为o，在给定中心词的情况下基于当前词向量计算上下文词出现的概率。（某些词会经常出现在中心词上下文，所以我们要不断改变词向量来尽可能最大化这些词出现在中心词上下文的概率）

#### 损失函数

我们希望预测固定大小M窗口内的上下文词，我们需要做的是计算出以前数据的似然：

> 似然是给定输出x时，关于参数θ的似然函数 $L(\theta|x)$ （在数值上）等于给定参数θ后变量x的概率： $L(\theta|x)=P(X=x|\theta)$ 。

$$Likelihood=L(\theta)=\displaystyle\prod_{t=1}^T\displaystyle\prod_{-m\le j \le m}logP(w_{t+j}|w_t;\theta)$$

我们需要做的是最大化我们在中心词周围看到的上下文的似然，改变一下似然函数，取平均对数似然。由于我们喜欢最小化我们的目标，所以再加个负号。从而得到目标函数（损失函数）

$$J(\theta)=-\frac{1}{T}L(\theta)=-\frac{1}{T}\displaystyle\prod_{t=1}^T\displaystyle\prod_{-m\le j \le m}logP(w_{t+j}|w_t;\theta)$$

#### 出现概率

当给出中心词时，上下文词出现的概率为 $P(o|c)=\frac{exp(u_o^Tv_c)}{\sum_{w\in V}exp(u_w^Tv_c)}$ 

> ①对于分子，点积是两个向量关系的相似性度量（因为同号相加，异号相减），所以这似乎是一个合理的想法
> ②对于分子，我们不想概率为负，所以取指数
> ③我们需要总和为1，所以分母为词汇表中每个不同单词和中心词的相似度之和

这就是softmax的一个例子，因此，那么我们便成功得到了损失函数，接下来计算损失函数梯度即可（推导略）

$$\frac{\partial}{\partial V_c} logP(o|c) = u_o-\displaystyle\sum_{x=1}^VP(x|c)u_x$$

对于所有使用softmax的模型，最终情况一般都会是观察值减去期望值，因此，我们的模型很好(\*^▽^\*)

#### SkipGram

(1)SG全称skip-gram，是根据中心词预测上下文词

(2)模型结构

①输入层： 接收一个one-hot张量1×vocab\_size作为网络的输入。

②隐藏层： 将张量V乘以中心词embedding张量vocab\_size×embed\_size ，并把结果作为隐藏层的输出，得到一个形状为1×embed\_size的张量，里面存储着当前句子中心词的词向量。

③输出层： 将隐藏层的结果乘以上下文词embedding张量embed\_size×vocab\_size，得到一个形状为1×vocab_size的张量。这个张量经过softmax变换后，就得到了使用当前中心词对上下文的预测结果。根据这个softmax的结果，我们就可以去训练词向量模型。

> Skip-gram在实际操作中，使用一个滑动窗口（一般情况下，长度是奇数），从左到右开始扫描当前句子。每个扫描出来的片段被当成一个小句子，每个小句子中间的词被认为是中心词，其余的词被认为是这个中心词的上下文。

#### CBOW

(1)CBOW全称Continues Bag of Words，是根据上下文词预测中心词

(2)模型结构

①输入层： 一个形状为C×V的one-hot张量，其中C代表上下文中词的个数，通常是一个偶数，我们假设为4；V表示词表大小，我们假设为5000，该张量的每一行都是一个上下文词的one-hot向量表示。

②隐藏层： 一个形状为V×N的参数张量W1，一般称为word-embedding，N表示每个词的词向量长度，我们假设为128。输入张量和word embedding W1进行矩阵乘法，就会得到一个形状为C×N的张量。综合考虑上下文中所有词的信息去推理中心词，因此将上下文中C个词相加得一个1×N的向量，是整个上下文的一个隐含表示。

③输出层： 创建另一个形状为N×V的参数张量，将隐藏层得到的1×N的向量乘以该N×V的参数张量，得到了一个形状为1×V的向量。最终，1×V的向量代表了使用上下文去推理中心词，每个候选词的打分，再经过softmax函数的归一化，即得到了对中心词的推理概率

#### 负例采样

(1)负例采样

由于softmax标准化要对所有分数求和，计算代价昂贵，所以提出了skip-gram负采样方法（训练二元逻辑回归来区分真实对和噪声对）

$$J_{neg-sample}(u_o,v_c,U)=-log\sigma(u_o^Tv_c)-\displaystyle\sum_{k\in \{K \ sampled \ indices \}}log\sigma(-u_k^Tv_c)$$

主要思路：我们取k个负样本（使用单词概率）最大化真实外部单词出现的概率。

![image-20240221144424736](https://gitee.com/mianmann/drawing-bed-warehouse/raw/master/img/image-20240221144424736.png)

关于计算选择某个词作为负样本的概率，可以使用随机选择。但作者给出了效果更好的公式： $p(w_i)=\frac{f(w_i)^{3/4}}{\sum_{j=0}^mf(w_j)^{3/4}}$ 

公式中， $f(w_i)$ 代表语料库中 $w_i$ 出现的频率。上述公式更加平滑，能够增加低频词的选取可能。

(2)层次化softmax

### GloVe

#### 对比前序方法

(1)第一类方法是基于统计并且依赖矩阵分解 (例如LSA，HAL) 。虽然这类方法有效地利用了全局的信息，它们主要用于捕获单词的相似性，但是对例如单词类比的任务上表现不好。

(2)第二类方法是基于浅层窗口 (例如，Skip-Gram和CBOW 模型) ，这类模型通过在局部上下文窗口通过预测来学习词向量。

#### 共现矩阵

我们用X表示word-word共现矩阵，其中 $X_{ij}$ 表示词j出现在词i上下文中的次数。令 $X_i=\displaystyle\sum_kX_{ik}$ 为任意词出现在词i的上下文次数。最后，令 $P_{ij}=(w_j|w_i)=\frac{X_{ij}}{X_i}$ 为词j出现在词i的上下文的概率。

> 对于庞大的语料库，这样会产生庞大的计算量，但这只是一次性的前期投入成本

#### 最小二乘法目标函数

回想一下 Skip-Gram 模型，我们使用 softmax 来计算词 j出现在词 i 的上下文的概率。

$$Q_{ij}=\frac{exp(u_j^Tv_i)}{\sum_{w=1}^W xp(u_w^Tv_i)}$$

训练时以在线随机的方式进行，但是隐含的全局交叉熵损失可以如下计算：

$$J=-\displaystyle\sum_{i \in corpus}\sum_{j \in context(i)}logQ_{ij}$$

同样的单词 i 和 j 可能在语料库中出现多次，因此首先将 i 和 j 相同的值组合起来更有效：

$$J=-\displaystyle\sum_{i=1}^W\displaystyle\sum_{j=1}^WX_{ij}logQ_{ij}$$

交叉熵缺失的一个显著缺点是因为Q在对数中，所以要求分布Q被正确归一化，但对整个词汇的求和的计算量是非常大的。因此，我们使用一个最小二乘的目标函数：

$$J=\displaystyle\sum_{i=1}^W\displaystyle\sum_{j=1}^WX_i(P_{ij}-Q_{ij})^2$$

其中  $P_{ij}=X_{ij}$  和  $Q_{ij}=exp(u_j^Tv_i)$ 是未归一化分布。这个公式带来了一个新的问题，$X_{ij}$经常会是很大的值，从而难以优化。一个有效的改变是最小化 P 和Q对数的平方误差：

$$J=\displaystyle\sum_{i=1}^W\sum_{j=1}^WX_i(log(P_{ij})-log(Q_{ij}))^2 \\ = \displaystyle\sum_{i=1}^W\sum_{j=1}^WX_i(u_j^Tv_i-logX_{ij})^2 \ \ \ \ \ $$

另外一个问题是权值因子 $X_i$不能保证是最优的。因此，我们引入更一般化的权值函数，我们可以自由地依赖于上下文单词：

$$J=\displaystyle\sum_{i=1}^W\displaystyle\sum_{j=1}^Wf(X_{ij}(u_j^Tv_i-logX_{ij}))^2$$

#### GloVe模型结论

GloVe模型仅对单词共现矩阵中的非零元素训练，从而有效地利用全局统计信息，并生成具有有意义的子结构向量空间。给出相同的语料库，词汇，窗口大小和训练时间，它的表现都优于Word2Vec，它可以更快地实现更好的效果，并且无论速度如何，都能获得最佳效果。

## 词向量和NER

### 词向量评估

#### 词向量内部评估

(1)词向量的内部评估是对一组由如Word2Vec或GloVe生成的词向量在特定的中间子任务 (如词类比) 上的评估。这些子任务通常简单而且计算速度快，从而能够帮助我们理解生成的的词向量。

(2)词类比

一个比较常用的内部评估的方法是词向量的类比。在词向量类比中，给定以下形式的不完整类比：

$$a:b ::c : ?$$

然后内部评估系统计算词向量的最大余弦相似度：

$$d=arg\displaystyle\max_i \frac{(x_b-x_a+x_c)^Tx_i}{||x_b-x_a+x_c||}$$

可以看到单词向量包含向量间距离的意义，这个指标有直观的解释。理想的情况下，我们希望  $x_b-x_a=x_d-x_c$  (例如，queen-king=actress-actor) 。这就暗含着我们希望  $x_b-x_a+x_c=x_d$  。因此，我们确定可以最大化两个词向量之间的归一化点积的向量$x_d$即可 (即余弦相似度) 。

> **余弦相似度**用向量空间中两个向量夹角的余弦值作为衡量两个个体间差异的大小。余弦值越接近1，就表明夹角越接近0度，也就是两个向量越相似，这就叫"余弦相似性"。

#### 词向量外部评估

对一组在实际任务中生成的词向量的评估，这些任务通常复杂而且计算速度慢。

### 词向量应用于外部任务

#### 外部任务范式

(1)很多 NLP 的外部任务都可以表述为分类任务。例如，给定一个句子，我们可以对这个句子做情感分类，判断其情感类别为正面，负面还是中性。相似地，在命名实体识别 (NER) ，给定一个上下文和一个中心词，我们想将中心词分类为许多类别之一。

(2)在一般的机器学习任务中，我们通常固定输入数据和目标标签，然后使用优化算法来训练权重 (例如梯度下降，L-BFGS，牛顿法等等) 。然而在NLP应用中，我们引入一个新的思想：在训练外部任务时对输入词向量进行再训练。下面我们讨论何时使用以及为什么要这样做。

#### 词向量再训练

在许多情况下，我们会基于内部任务去训练得到词向量，进而再用于外部任务，很多时候这些预训练的词向量在外部评估中表现良好。但是，这些预训练的词向量在外部评估中的表现仍然有提高的可能。当然，重新训练也存在着一定的风险。

> 如果训练数据集很小，就不应该对单词向量进行再训练。如果训练集很大，再训练可以提高性能。

#### 窗内容分类

一般来说，较窄的窗口大小会导致在句法测试中更好的性能，而更宽的窗口会导致在语义测试中更好的性能。

### 命名实体识别(NER)

#### 基本任务

通过标记单词，找到文本中的名字并对其进行分类

#### 主要思路

在相邻单词的上下文窗口中对每个单词进行分类：在手工标记的数据上训练逻辑分类器，根据窗口中单词向量的级联对每个类别的中心单词 $yes/no$ 进行分类。（实际上，我们通常使用多类别softmax，但我们试图保持简单）

#### 如何分类

我们有一个训练数据集，由样本组成 $\{x_i,y_i\}^N_{i=1}$ ， $x_i$ 是输入的文本， $y_i$ 是对应的标签（监督学习）

> 典型的ML/stats softmax分类器：
> ①学习的参数θ只是W的元素（而不是具有稀疏符号特征的输入表示x）
>
> ②分类器给出了线性决策边界，这可能是有限的

这里使用神经网络分类器，多次重新表示和组合数据并分类，从而提供非线性分类器

![image-20230908191815973](https://gitee.com/mianmann/drawing-bed-warehouse/raw/master/img/image-20230908191815973.png)



## 依存结构和依存分析

### 依存结构

与编译器中的解析树类似，NLP中的解析树是用于分析句子的句法结构。使用的结构主要有两种类型——短语结构和依存结构。短语结构文法使用短语结构语法将词组织成嵌套成分。后面的内容会展开对它做更详细的说明。我们现在关注依存语法。

句子的依存结构展示了单词依赖于另外一个单词 (修饰或者是参数)。词与词之间的二元非对称关系称为依存关系，描述为从head (被修饰的主题) 用箭头指向dependent (修饰语)。一般这些依存关系形成树结构，他们通常用语法关系的名称 (主体，介词宾语，同位语等)。

> CFG：Context-Free Grammers（上下文无关语法）

### 依存语法

(1)依存语法是给定一个输入句子S，分析句子的句法依存结构的任务。依存句法的输出是一棵依存语法树，其中输入句子的单词是通过依存关系的方式连接。

正式一点定义，依存语法问题就是创建一个输入句子的单词 $S=w_0w_1\dots w_n$ (其中$w_0$是 ROOT) 到它的依存语法树的映射图G。最近几年提出了很多以依存句法为基础的的变体，包括基于神经网络的方法，我们将会在后面介绍。

> 我们通常会添加一个假ROOT，这样每个单词都恰好依赖于一个其他节点 

(2)依存语法中的子问题

确切地说，在依存语法中有两个子问题：
①学习：给定用依赖语法图标注的句子的训练集D，创建一个可以用于解析新句子的解析模型M
②分析：给定分析模型M和句子S，根据M得到S的最优依存语法图 

> 「学习」问题是创建一个可以根据转移历史来预测状态机中的下一个转换的模型。
>
> 「分析」问题是使用在学习问题中得到的模型对输入句子构建一个最优的转移序列。

### 依存分析

(1)通过为每个单词选择它所依赖的其他单词（包括ROOT）来分析句子，这通常有一些限制：
①只有一个单词依赖于ROOT
②不希望循环A→ B、 B→ A
③这使依赖关系成为一棵树
④箭头是否可以交叉（非投影）

![image-20230910160743232](https://gitee.com/mianmann/drawing-bed-warehouse/raw/master/img/image-20230910160743232.png)

> 投射解析：当单词按线性顺序排列时，不存在交叉依赖弧

(2)依存分析方法

①依存分析的方法有：Dynamic programming、Graph algorithms、Constraint Satisafaction、Transition-based parsing

②Transition-based parsing：Transition-based 依存语法依赖于定义可能转换的状态机，以创建从输入句到依存句法树的映射。

#### Greedy Deterministic Transition-Based Parsing

(1)状态：对任意句子 $S=w_0w_1\dots w_n$ ，一个状态可以描述为一个三元组 $c=(\sigma,\beta,A)$ 

(σ是S中单词的堆，β是S中单词的缓冲区， A是依存弧集合，对于一组形式为 $(w_i,r,w_j)$ 的依存弧，其中r是二者的依存关系)

(2)转移：在状态之间有三种不同类型的转移

①SHIFT：移除在缓冲区的第一个单词，然后将其放在堆的顶部 (前提条件：缓冲区不能为空)
②Left-Arc：向依存弧集合A中加入一个依存弧 $(w_i,r,w_j)$ ，其中$w_i$是堆顶的第二个单词，$w_j$是堆顶部的单词，从堆中移除$w_i$（前提条件：堆必须包含两个单词以及$w_i$不是 ROOT ）
③Right-Arc：向依存弧集合A中加入一个依存弧 $(w_i,r,w_j)$ ，其中$w_i$是堆顶的第二个单词，$w_j$是堆顶部的单词，从堆中移除$w_j$（前提条件：堆必须包含两个单词）

![image-20230910163900211](https://gitee.com/mianmann/drawing-bed-warehouse/raw/master/img/image-20230910163900211.png)

![image-20230910164012601](https://gitee.com/mianmann/drawing-bed-warehouse/raw/master/img/image-20230910164012601.png)

> 每个动作都由判别分类器（例如softmax分类器）对每个合法动作进行预测

#### 神经依存解析器

定义神经网络的输入是灵活的。对给定句子 $S$的特征包含一些子集：

① $S_{word}$ ：在堆σ的顶部和缓冲区β的S中一些单词的词向量 (和它们的依存)
② $S_{tag}$ ：在S中一些单词的词性标注 ( POS )。词性标注是由一个离散集合组成： $P={NN,NNP,NNS,DT,JJ,\dots}$ 
③ $S_{label}$ ：在S中一些单词的依存标签。依存标签是由一个依存关系的离散集合组成： $L={amod,tmod,nsubj,csubj,dobj}$ 

对每种特征类型，我们都有一个对应的将特征的 one-hot 编码映射到一个d维的稠密的向量表示的嵌入矩阵。

## 语言模型和RNN

### 语言模型定义

(1)语言模型任务是根据给定的单词序列计算下一个单词的概率分布，完成这样的任务的系统就被称作语言模型

(2)也可以认为语言模型是一个为文本分配概率的系统，例如文本 $x^{(1)},\cdots,x^{(T)}$ 的概率是
![image-20230910171935861](https://gitee.com/mianmann/drawing-bed-warehouse/raw/master/img/image-20230910171935861.png)

### n-gram语言模型

(1)一个n-gram是由n个连续单词组成的一块文本，收集不同n-gram的频率统计数据，并使用这些数据预测下一个单词

(2)思路

①首先我们做一个Markov假设， $x^{(t+1)}$ 仅取决于其前面的n-1个单词
![image-20230910173349467](https://gitee.com/mianmann/drawing-bed-warehouse/raw/master/img/image-20230910173349467.png)

②如何得到这些n-gram和(n-1)-gram的概率：

在大型文本语料库计算它们，统计概率近似
$$\approx\frac{count(x^{(t+1),x{(t)},\cdots,x^{(t-n+2)}})}{count(x^{(t)},\cdots,x^{(t-n+2)})}$$

(3)存在的问题：

- 稀疏性问题：当我们需要的预测条件文本根本没有出现在语料库中，解决方法如下
  ①添加小的$\sigma$扰动因子给每一个语料库中的单词（smoothing）
  ②以需要文本的前一部分作为预测条件（backoff）
  
- 存储问题：需要存储语料库中所有n-gram的数量，增加n或增加语料库都会增加模型大小

- 当生成长文本时，文本虽然语法连贯但会变得牛头不对马嘴

### 基于固定窗口的神经网络语言模型

![image-20230910180111928](https://gitee.com/mianmann/drawing-bed-warehouse/raw/master/img/image-20230910180111928.png)

> ①改进：没有稀疏性问题，不需要存储所有的n-gram
>②问题：窗口太小，需要放大，但是放大窗口也会放大模型

### RNN

(1)RNN全称：Recurrent Neural NetWorks（递归神经网络）

(2)核心思路：反复的应用相同的权重

(3)优缺点：

①优点：可以处理任何长度的输入，对于较长的输入上下文，模型大小不会增加
②缺点：递归计算很慢，很难从许多步骤后访问信息

(4)前向传播：

![image-20230911151645445](https://gitee.com/mianmann/drawing-bed-warehouse/raw/master/img/image-20230911151645445.png)

①获取一个大的文本语料库，它是一个单词序列
②将单词序列输入RNN模型，计算每一个步骤t的输出分布，上一步的输出是下一步的输入
③步骤t上的损失函数是预测的概率分布 $\widehat y^{(t)}$ 和真正的下一个单词的概率分布 $y^{(t)}$ （ $x^{(t+1)}$ 的one-hot编码）之间的交叉熵
$$J^{(t)}(\theta)=CE(y^{(t)},\widehat y^{(t)})=-\displaystyle\sum_{w \in V}y_w^{(t)}log\widehat y_w^{(t)}=-logy_{x_{t+1}}^{(t)}$$

④将其平均化，已获得整个训练集的整体损失

![image-20230911141359691](https://gitee.com/mianmann/drawing-bed-warehouse/raw/master/img/image-20230911141359691.png)

然而，一次计算整个语料库的损失梯度和代价太昂贵了，在实践中，我们将 $x^{(1)},\cdots,x^{(T)}$ 作为句子或文章。计算一个句子（实际上是一批句子）的损失，计算梯度并更新权重，对新一批句子重复。

(5)反向传播

> 多变量链式法则：
> ![image-20230911142231272](https://gitee.com/mianmann/drawing-bed-warehouse/raw/master/img/image-20230911142231272.png)

![image-20230911143810303](https://gitee.com/mianmann/drawing-bed-warehouse/raw/master/img/image-20230911143810303.png)

(6)RNN中的梯度消失和梯度爆炸：

- 梯度消失：如果权重很小，由推导得梯度为权重的距离数指数，所以距离一长就会导致无法预测类似的长距离依赖关系（即无法在长距离保存信息）![image-20230911152125342](https://gitee.com/mianmann/drawing-bed-warehouse/raw/master/img/image-20230911152125342.png)

- 梯度爆炸：同理，如果权重很大会导致梯度很大，进而会导致糟糕的更新，梯度下降时会走很大的一步

  解决方案：梯度裁剪，当梯度大于某一个阈值，就在应用SGD更新之前缩小它

### 语言模型的评估

(1)困惑度（perplexity）

$$perplexity=\displaystyle\prod_{t=1}^T(\frac{1}{P_{LM}(x^{(t+1)}|x^{(t)},\cdots,x^{(1)})})^{\frac{1}{T}}$$

(这其实就等于交叉熵损失的指数)

## LSTM和机器翻译

### LSTM

(1)LSTM（Long Short-Term Memory RNNs）是Hochreiter和Schmidhuber在1997年提出的一种RNN，用于解决消失梯度问题

(2)在步骤t中，这里有一个隐藏单元$h^{(t)}$和一个记忆单元$c^{(t)}$

- 它们均为长度为n的向量

- 记忆单元用来存储长期信息

- LSTM可以从记忆单元中读取、消除和写入信息，读取、消除、写入信息的选择由三个相应的门控制：

  ①门也是长度为n的向量
  ②在每个时间步长上，门的每个元素可以是打开的（1）、关闭的（0）或介于两者之间的某个位置
  ③门是动态的，它们的值是根据当前上下文计算的

详见下图：

![image-20230912130422464](https://gitee.com/mianmann/drawing-bed-warehouse/raw/master/img/image-20230912130422464.png)

(3)LSTM如何解决的梯度消失问题

- LSTM体系结构使RNN更容易在多个时间步长上保存信息

  > 例如：如果对于单元维度忘记门被设置为1并且输入门被设置成0，则该单元的信息被无限期地保留。

- 梯度消失/爆炸问题所有神经网络在深度大的共有的问题，如今有很多深度架构可以解决这一问题：

  ①LSTM：存储长期记忆
  ②ResNet：跳过连接
  ③DenseNet：直接将每一层连接到未来的所有层

### 双向RNN

(1)双向RNN全称：Bidirectional RNNs

(2)将Forward RNN和Backward RNN组成一个合并的隐藏层：

![image-20230912133953378](https://gitee.com/mianmann/drawing-bed-warehouse/raw/master/img/image-20230912133953378.png)

双向RNN仅适用于可以访问整个输入序列的情况，他并不适用于语言模型，因为在语言模型中，我们只有输入的上文

### 多层RNN

(1)多层RNN全称：Multi-layer RNNs

(2)使RNN可以在多个维度上深入，维度较低的RNN应计算较低级别的特征，而较高的RNN则应计算较高级别的特征

### 机器翻译

(1)统计机器翻译（SMT）：从数据中学习概率模型

(2)神经机器翻译（NMT）：是一种使用单个端到端神经网络进行机器翻译的方法。网络架构被称为序列-序列模型（也称为seq2seq），它涉及两个RNN

(3)机器翻译的评估

①BLEU（双语评估研究）：将机器书写的翻译与一个或多个人工书写的翻译进行比较，并根据n-gram精度加上对太短的系统翻译的惩罚来计算相似性得分

(4)优缺点

①优点：与SMT相比性能更好，仅需端到端优化，需要的人工工作量更小
②缺点：不太可解释，难以控制

## 解码和Attention机制

### 语言模型的解码

> △：在讲义中这部分内容是神经机器翻译NMT中，我认为这个适用于所有语言模型所以就单列出来了

(1)贪心解码：每一步都取最可能的单词，用前一步的输出作为下一步的输入（存在问题：有一个预测偏差，会导致后面的生成不可挽回，最终结果不尽人意）

(2)穷举解码：顾名思义，尝试计算每一个可能的序列y，取概率最大的一个（显而易见，复杂性太高）

(3)波束搜索解码：在解码器的每一步上，跟踪k个最可能的假设，假设的得分是其对数概率，我们搜索得分较高的假设，跟踪每一步的前k个假设

### Attention机制

#### seq2seq with attention

(1)序列-序列模型（又叫seq2seq模型或Encoder-Decoder模型）：一个神经网络接受输入并产生神经表示（编码），另一个网络基于该神经表示产生输出（解码）

(CNN、RNN、LSTM、AE等其实都可以归为seq2seq模型)

>  注意力机制最早用在seq2seq模型上，这里我们也以seq2seq模型中的注意力机制来分析

(2)核心思想：在解码器的每一步上，使用与编码器的直接连接来关注源序列的特定部分

(3)理解

在文本处理领域，一类常见的任务就是由一个句子（Source）生成另一个句子（Target）

$$Source=(x_1,x_2,\cdots,x_m)$$
$$Target=(y_1,y_2,\cdots,y_m)$$
$$C=F(x_1,x_2,\cdots,x_m)$$

Source经过Encoder生成中间的语义编码C，C经过Decoder时，先根据$C$生成 $y_1$ ，再根据 $C,y_1$ 生成 $y_2$ ，以此类推

传统的循环神经网络中，$y_1,y_2,y_3$的计算都是基于同一个C。深入思考一下，发现这可能并不是最好的方案，因为 Source 中不同单词对 $y_1,y_2,y_3$ 的影响是不同的，所以，很自然地就有了如下思路：对 $y_1$ 使用$C_1$，对 $y_2$ 使用 $C_2\cdots$ 

$$C_1=a_{11}f(x_1)+a_{12}f(x_2)+a_{13}f(x_3)$$ $$C_2=a_{21}f(x_1)+a_{22}f(x_2)+a_{23}f(x_3)$$
$$C_3=a_{31}f(x_1)+a_{32}f(x_2)+a_{33}f(x_3)$$

权重向量$a_t$即为下图中的Attention Output，接下来会讲具体是怎么计算的

![image-20230913094540714](https://gitee.com/mianmann/drawing-bed-warehouse/raw/master/img/image-20230913094540714.png)

(4)对步骤的具体计算

在第t个时间步长（生成第t个单词时）下，会进行如下运算：

编码器的隐藏层： $h_1,\cdots,h_N$ 
解码器的隐藏层： $s_t$ 
那么我们的注意力得分 $e^t=[s_t^Th_1,\cdots,s_t^Th_N]$ 
那么我们的注意力分布 $a^t=softmax(e^t)$ 
那么我们的注意力输出 $a_t=\displaystyle\sum_{i=1}^Na_i^th_i$ （此即为权重向量）

最后将注意力输出与解码器的隐藏层连接起来，按照无注意力的seq2seq模型处理

(5)意义

①注意力机制显著提高了NMT的性能
②提供了一个更加“拟人化”的MT模型
③解决了瓶颈问题
④有助于消除梯度问题
⑤提供了可解释性

#### attention variants

(1)种类：

①Basic dot-product attention： $e_i=s^Th_i$ 
②Multiplicative attention： $e_i=s^TWh_i$ 
③Reduced-rank multiplicative attention： $e_i=s^T(U^V)h_i=(Us)^T(Vh_i)$ 
④Additive attention： $e^T=v^Ttanh(W_1h_i+W_2s)$ 

(2)注意力机制是一种通用的学习技巧，更通用定义为：给定一组向量值和一个向量查询，注意力是一种根据查询计算值的加权和的技术。

## Transformer

### 模型回顾

问题：RNN需要经过k步才能对远距离的单词进行交互，例如

![image-20230913143154300](https://gitee.com/mianmann/drawing-bed-warehouse/raw/master/img/image-20230913143154300.png)

这里的was是chef的谓语，二者的关系十分紧密，但是使用线性顺序分析句子会导致如果was和chef的距离较远，它们会难以交互（因为梯度问题）

### Self Attention

#### 键值对注意力

(1)我们可以将注意力视为在键值存储中执行模糊查找，在一个key-value查找表中，查询会对所有keys进行软匹配，然后相应的value将乘以权重并求和

![image-20230914123856025](https://gitee.com/mianmann/drawing-bed-warehouse/raw/master/img/image-20230914123856025.png)

(左边是强匹配，右边是软匹配)

(2)理解

①键值对Attention最核心的公式如下$$Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}}V)$$

如果这个公式很难理解，那么我们先从self-attention的原始形态入手，原始形态为 $softmax(XX^T)X$ 

②下面解读这个原始形态的公式：
$XX^T$ 是输入矩阵$X$中每一个词向量与自己的内积，我们已经知道向量的内积表示向量的相关性，所以$XX^T$相当于一个类共现矩阵，再用softmax归一化成权重，那么便得到了一个输入矩阵$X$的相关度矩阵，可以将其理解为一个键值对表。将这个键值对表再$X$乘积，就实现了软匹配键值对表注意力

③接下来解释一下  $softmax(\frac{QK^T}{\sqrt{d_k}}V)$  ：

$Q,K,V$ 是什么？ $XW^Q=Q,XW^K=K,XW^V=V$ ，可以看到它们其实就是调整后的$X$，为什么我们不直接使用$X$，因为调整矩阵$W$是可以训练的，起到一个缓冲的效果

$\sqrt{d_k}$ 的意义在于，假设 $Q,K$ 里的元素的均值为0，方差为1，那么  $A^T=Q^TK$ 中元素的均值为0，方差为d。当d变得很大时，$A$中的元素的方差也会变得很大，如果$A$中的元素方差很大，那么  $softmax(A)$ 的分布会趋于陡峭(分布的方差大，分布集中在绝对值大的区域)。因此$A$中每一个元素除以 $\sqrt{d_k}$ 后，方差又变为1。这使得 $softmax(A)$ 的分布“陡峭”程度与d解耦，从而使得训练过程中梯度值保持稳定（详见后续 缩放点积）

(3)问题与解决

①如何加入考虑序列顺序：由于自我注意不是建立在有序信息中的，我们需要在键、查询和值中对句子的顺序进行编码

我们将每个序列索引表示为向量$p_i$，然后将位置向量添加到我们的输入中 $\widetilde x_i=x_i+p_i$

- 正弦曲线的位置向量：连接不同周期的正弦函数成为向量
  (随着周期开始可以推断出更长的序列，但是不能学习，不能外推)
- 从头学习的位置向量：学习一个矩阵$p$，让每一个位置向量$p_i$为该矩阵的一列
  (具有灵活性，每个位置都需要学习以适应数据，绝对不能外推到输入序列以外，大多数系统都使用此方法)

②如何加入非线性（self-attention深度学习的输出结果是线性的，只是加权平均数）

在自注意力机制中加入非线性：添加一个前馈网络来对每个输出向量进行后处理（可以提供更多的非线性和学习能力）

$$m_i=MLP(output_i)=W_2*ReLU(W_1output_i+b_1)+b_2$$

③如何防止窥探未来（Attention Mask）：（因为Transformer在训练时是不应该提前看到后面的信息的，这属于作弊，作弊会导致用“你好世界上的人”训练出来的模型在输入“你好”时可能不会生成“世界上的人 ”）通过将注意力得分设置为 $-\infty$ 来掩盖对未来单词的注意力

![image-20230916191845675](https://gitee.com/mianmann/drawing-bed-warehouse/raw/master/img/image-20230916191845675.png)

(5)总结：self-attention需要位置表示、非线性和掩蔽未来

![image-20230916193038886](https://gitee.com/mianmann/drawing-bed-warehouse/raw/master/img/image-20230916193038886.png)

右边的流程图即为self-attention的全过程

### Transformer模型

#### Multi-head Self Attention

(1)为什么要使用multi-head self-attention

模型在对当前位置的信息进行编码时，会过度的将注意力集中于自身的位置（虽然这符合常识）而可能忽略了其它位置，因为当文本很长时，自身的权重过高会导致分配给其他位置的权重较小，可能会存在某些位置权重矩接近于0的情况（即被忽略），这是我们不想看到的

(2)主要思路

让我们回忆一下这个公式

$$output_l=softmax(XQ_lK_l^TX^T)XV_l$$

我们将通过多个Q、K、V矩阵定义多个注意力头 ，每个注意力头都独立的进行注意力感应，这里我们定义 $Q_l,K_l,V_l \in R^{d \times\frac{d}{h}}$ （ $R^{m\times n}$ 表示一个$m \times n$的矩阵，h是注意力头的数量，$l$的范围从1到h）

最后再将h个输出结果合并，即可得到一个$d\times d$的输出矩阵

#### Scaled Dot Product

(1)为什么要使用点积缩放

当维数d变得很大时，向量的点积往往会变得非常大，所以我们需要将attention scores除以注意力头的数量，即d/h

$$output_l=softmax(\frac{XQ_lK_l^TX^T}{\sqrt{d/h}})XV_l$$

#### Encoder-Decoder

(1)残差连接和层规一化，通常被一起作为"Add & Norm"

(2)残差连接（即残差网络思想）：可以帮助模型训练的更好

![image-20230917141946899](https://gitee.com/mianmann/drawing-bed-warehouse/raw/master/img/image-20230917141946899.png)

(3)层规一化：可以帮助模型训练的更快

通过归一化，减少隐藏向量值的无信息变化，我们得到

![微信图片_20230917143010](https://gitee.com/mianmann/drawing-bed-warehouse/raw/master/img/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20230917143010.png)

(5)The Transformer Decoder

![image-20230917143741073](https://gitee.com/mianmann/drawing-bed-warehouse/raw/master/img/image-20230917143741073.png)

- Transformer解码器由一堆Transformer解码器块组成
- 每个解码器块包括:
  ①self-attention
  ②add&norm
  ③feed-forward
  ④add&norm

(6)The Transformer Encoder

![image-20230917144021835](https://gitee.com/mianmann/drawing-bed-warehouse/raw/master/img/image-20230917144021835.png)

与解码器不同的是，编码器将去除了self-attention中的mask机制

(7)总结

![image-20230917145254816](https://gitee.com/mianmann/drawing-bed-warehouse/raw/master/img/image-20230917145254816.png)

回顾一下，在机器翻译中我们用双向模型处理源句子，并用单向模型生成目标。在Transformer中的解码器中，我们可以进行修改，对编码器的输出进行交叉关注，于是便得到了右侧的模型（是我们Transformer的完整模型）

模型中左边为编码器，右边为解码器，解码器中第一个Masked Multi-Head Attention是直接根据输入的X矩阵的到Q、K、V，而第二个Multi-Head Attention是根据编码器的输出得到K、V，但Q仍然是由编码器的输出得到

#### 缺点与问题

(1)自我关注的二次运算会导致计算的增长 $O(n^2d)$

- 相关研究：Linformer
  关键思想：将序列长度维度映射到值、键的低维空间

(2)简单的绝对指数使我们能做的做好的表示位置的指数吗

> 推荐一篇很通俗易懂的讲解https://blog.csdn.net/Tink1995/article/details/105080033

## 预训练

### 子词模型

#### 前置知识

(1)词法知识介绍

- 语音学是音流无争议的物理学
  语音体系假定了一组或多组独特的、分类的单元（音素）

- 传统上，词素是最小的语义单位（例如$ate,ly,able$这种），但如今我们需要讨论比单词粒度更细的模型以处理大量的开放词汇（巨大的、无限的单词空间）
  例如：ooooooooooops!、imma go等非正式的拼写（这种情况在其他语言中更甚）

(2)字符级模型：词嵌入可以由字符嵌入组成，所有的语言处理均建立在字符序列上，不考虑word-level

#### 子词模型

(1)NLP中的子词模型包含了一系列关于单词级别以下结构的推理方法（单词、字符、字节的组成部分）
(在训练和测试时，每个单词被分成一系列已知的子单词)

(2)子词模型的两种趋势

①与Word级模型相同的架构，但使用更小的单元：word pieces
②混合架构：主模型使用单词，其他使用字符级

(3)字节对编码（BPE）

- 最初是一个压缩算法：最频繁的字节-->一个新的字节
- 字节对编码是定义子单词词汇表的一种简单有效的策略
  ①从只包含字符和“词尾”符号的词汇表开始。
  ②使用文本语料库，找出最常见的相邻字符“a，b”；添加“ab”作为一个子词。
  ③用新的子字替换字符对的实例，重复，直到达到所需的人声大小。
- 最初用于NLP中的机器翻译，现在在预训练的模型中使用了类似的方法

(4)普通单词最终成为子单词词汇的一部分，而稀有单词则被划分为组成部分（在最坏的情况下，单词被拆分为与字符一样多的子单词）

![image-20230918150809682](https://gitee.com/mianmann/drawing-bed-warehouse/raw/master/img/image-20230918150809682.png)

### 预训练

#### 预训练解读

(1)有一个问题，我们网络中的大多数参数都是随机初始化的，这样会导致模型训练效果不好、容易过拟合等各种问题

(2)“预训练”的做法一般是将大量低成本收集的训练数据放在一起，经过某种预训练方法去学习其中的共性，然后将其中的共性“移植”到特定任务的模型中，再使用相关特定领域的少量标注数据进行“微调”，这样的话，模型只需要从”共性“出发，去“学习”该特定任务的“特殊”部分即可。

> 其实word2vec就是一个专注于词嵌入的预训练模型

(3)预训练&微调范式

①第一阶段：预训练，需要大量文本，学习共性
②第二阶段：微调，不需要太多标签，适应我们的任务

#### 预训练架构

(1)Encoder：例如BERT

(2)Decoder：例如GPT

### 预训练模型

#### ELMO

(1)概念

- Contextualized Word Embedding（情境化Embedding）：同一个word在不同的位置会视作不同的token
- ELMO（Embedding from Language Model）：使用RNN-based语言模型来训练，预测下一个token或上一个token，拿在正向和反向RNN的隐含层拼接作为Contextualized Embedding（不需要label）

#### BERT

(1)概念

- Bidirectional Encoder Representations from Transformers：即为Transformer的Encoder架构（不需要label）
- BERT在做什么事情：输入一个句子进去，输出每一个词的embedding
- BERT的IDEA： 传统的语言模型就是单向的信息，而ELMO是正向+反向（参数量变成了之前单向的两倍，也是直接考虑双向的两倍，而且对于某些任务例如QA不合理）这比直接考虑双向模型更差，因为双向模型能够在同一个layer中直接考虑左边和右边的context。所以BERT直接考虑双向。

(2)过程

BERT主要由两个阶段组成，分别是Pre-training以及Fine-Tuning

- Pre-Training：

  ①MLM：输入的句子有随机15%的概率被置换成[MASK]，BERT需要做的是猜测[MASK]应该是哪些词汇（把MASK的部分输出的Embedding输入到一个线性的多分类器中来预测单词）

  (但是mask wordpiece的做法也被后来（ERNIE以及SpanBERT等）证明是不合理的，没有将字的知识考虑进去，会降低精度，于是google提出了Whole Word Masking（WWM）的模型)

  ②NSP：给BERT两个句子，BERT来判断这两个句子是否应该被接在一起（[SEP]用来隔开两个句子，[CLS]用来表示需要输出到线性二分类器的位置）

  (注意两个方法需要同时使用)

- Fine-Tuning：相似度任务、推理任务、QA任务、文本分类

  ①BERT的每一个Layer对应着一个任务，越靠近输入的Layer做越简单的任务，越靠近输出的Layer做越困难的任务。

  ②使用BERT做后续任务时，可以学习每一层得到的Embedding哪个更有用

> 具体详见：https://zhuanlan.zhihu.com/p/103226488

#### GPT

(1)Generative Pre-Training：规模巨大，就是Transformer的Decoder

(2)力大飞砖，数据巨大、模型巨大，从而产生了很多奇迹的效果

