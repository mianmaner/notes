### ViT

将Transformer用于CV的关键问题：怎么将2d的图片数据变成1d的序列数据（直接将像素点作为序列元素然后平铺会导致序列元素很大，此时计算自注意力的开销非常大）

背景：之前的论文主要有：先通过其他网络将图片数据变为16*16的表征向量再输入到Transformer、提出轴自注意力机制从两个维度上进行自注意力等方法

**ViT** 力求尽可能作出最小的改动。做法是：将图片分成很多patch（例如16*16像素区域），每个patch经过一个fc层得到一个一维向量（作为一个word）。很简单的想法，其实之前已经有论文提出过（只不过是很小的patch和CIFAR-10数据集），但是ViT这篇论文第一次证明在大规模数据集上超过CNN（准确来说就是我们有钱）

> 对于卷积神经网络有两个归纳偏置信息
> ①locality：靠的越近的东西相关性越强 
> ②translation equirance：平移等变性，先做平移还是卷积结果都一样

该论文指出Transformer没有这两个归纳偏置信息，但大规模预训练后发现效果已经超越具有归纳偏执信息的CNN

![image-20240312214021881](https://gitee.com/mianmann/drawing-bed-warehouse/raw/master/img/image-20240312214021881.png)