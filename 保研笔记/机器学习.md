#### 基础知识

##### 1. 分类、回归和聚类

① 分类：预测结果是离散值（逻辑回归、LDA、SVM、ANN、决策树、KNN等）
② 回归：预测结果是连续值
③ 聚类：预测结果是分组值

##### 2. 学习方式

① 监督学习（分类与回归）：有标记信息
② 无监督学习（聚类）：无标记信息
③ 半监督学习：同时使用标记数据和未标记数据

##### 3. 偏差、方差和噪声

偏差：度量了学习算法的期望预测和真实结果的偏离程度，即刻画了算法的拟合能力
方差：度量了同样大小的训练集的变动所导致的学习性能的变化
噪声：表达了当前任务上任何学习算法所能达到的期望泛化误差的下界

> 经验风险：度量预测模型与训练数据的误差
> 结构风险：描述模型的某些性质，以避免过拟合
>
> 低偏差代表经验风险最小化
> 低方差代表结构风险最小化

##### 4. 泛化能力

对新鲜样本的适应能力

##### 5. 评估方法

交叉验证：将整个训练数据划分成 n 份，选取其中一份作为验证数据集，其余 n-1 份作为训练数据集，进行轮换让每一份数据都有机会作为验证数据集

##### 6. 评估指标

混淆矩阵：
True Positives(TP)    False Negtives(FN)
False Positives(FP)   True Negtives(TN)

查准率（Precision）：预测成功的正例占预测正例的比率，即$\frac{TP}{TP+FP}$
查全率（Recall）：预测成功的正例占真实正例的比率，即$\frac{TP}{TP+FN}$

P-R 曲线：纵坐标查准率，横坐标查全率
F1 得分：$\frac{2TP}{2TP+FP+FN}$

ROC（曲线）：纵坐标为查全率，横坐标为预测失败的反例占真实反例的比率
AUC：ROC曲线下的面积

##### 7. 熵

交叉熵：主要用于度量两组数据分布间的差异性（公式：$H(x,y)=-\sum P(x)logQ(x)$）

信息熵：主要用于度量样本集合纯度（公式：$H(x)=-\sum P(x)logP(x)$）

##### 8. 激活函数和优化函数

见博客笔记

#### 线性回归

目标为最小化$J(\theta)=\frac{1}{2}\sum_{i=1}^{n}(y^{(i)}-\theta^Tx^{(i)})^2$（这个式子就是最小二乘法）

#### 非线性回归

#### 逻辑回归

逻辑回归不是回归，是经典的二分类算法！(将连续的数值转化为对应的区间然后判断)

> 例如 Sigmoid 函数 $g(z)=\frac{1}{1+e^{-z}}$
> 效果：将任意的输入映射到了[0,1]区间

损失函数：交叉熵-信息熵

多分类逻辑回归：实际上是多个二分类任务

线性判别分析（LDA）：使用超平面切分两类样本，使得两类样本类内距最小，类间距最大

#### 聚类

**K-MEANS 算法**

想要得到簇的个数，需要指定K值

① 先随机初始化k个点，记作分类点
② 计算每一个数据点与 k 个分类点的距离，和最近的分类点归为一簇
③ 再更新各簇分类点为各簇的质心
重新进行步骤②，不断迭代

**DBSCAN 算法**

$\epsilon$-领域的距离阈值：设定的半径 r

对每个点都判断一遍，若某点p在点q的r领域内，且q是核心点，则p-q直接密度可达

噪声点：不属于任何一个簇的点，从任何一个核心点出发都是不可达的

#### 决策树

(1) 树模型：
从根节点一步步走到叶子结点
所有的数据最终都会落到叶子结点
既可以做分类也可以做回归

(2) 树的组成
根节点：第一个选择点
非叶子节点与分支：中间过程
叶子结点：最终的决策结果

(3) 如何切分：信息熵、GINI、信息增益率等

(4) 剪枝策略
原因：决策树过拟合风险很大
(可以理解为决策树性能太强，太能分了)

#### 集成学习

(1)Bagging：训练多个分类器取平均（代表算法：随机森林）

- 随机森林：是一种策略，适用于多分类、回归等任务

  随机：数据采样随机、特征选择随机
  森林：很多个决策树放在一起

(2)Boosting：从弱学习器开始加强，通过加权来进行训练

- AdaBoost算法

  AdaBoost会根据前一次的分类效果调整数据权重
  解释：如果某一个数据在这一次分错了，下一次给予它更大的权重

(3)Stacking：聚合多个分类或回归模型 

- 堆叠：可以堆叠各种各样的分类器（KNN,SVM,RF等）

#### 支持向量机

简单点讲，SVM就是一种二分类模型，他的基本模型是的定义在特征空间上的间隔最大的线性分类器，SVM的学习策略就是间隔最大化

$\omega^Tx+b=0$ 决定一个超平面，我们先选择分离两类数据的两个平行超平面，然后最大化间隔 $\frac{2}{||w||}$

训练数据集的样本点中与分离超平面距离最近的数据点称为支持向量，支持向量是使 $y_i(X_i^TW+b)=1$ 的点，在决定最佳超平面时只有支持向量起作用，而其他数据点并不起作用，如果移动非支持向量，甚至删除非支持向量都不会对最优超平面产生任何影响

对偶问题：用 KKT 条件求解

软间隔就是给目标函数加上一个偏置项（惩罚项）

非线性 SVM 需要利用核函数将低微映射到高维可分

#### 贝叶斯分类器

贝叶斯公式

判决函数：最大似然

#### 主成分分析 PCA

1.主要用途：数据压缩，数据可视化，维度压缩

2.目标：删去重复特征，去除冗余特征，建立尽可能少的特征，使得这些新特征两两不相关

3.思路：通过线性组合现有特征实现组合后的方差最大化
